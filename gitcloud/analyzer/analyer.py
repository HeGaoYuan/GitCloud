#!/usr/bin/env python3
"""
Enhanced Resource Analyzer Agent for GitCloud
----------------------------------------------
Intelligently analyzes GitHub repositories to determine:
1. Project type classification (comprehensive software domain taxonomy)
2. Required cloud services (based on Tencent/Alibaba cloud products)
3. Resource specifications for each service
"""

import os
import re
import json
import subprocess
import tempfile
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
from urllib.parse import urlparse

from .cloud_service_spec import (
    CloudServiceRequirement,
    ServiceRequirement,
    ProjectType,
    CloudServiceType,
    get_default_services_for_project
)
from .resource_spec import ResourceSpec
from .github_fetcher import GitHubFetcher


# ANSI color codes for log output
class LogColors:
    INFO = '\033[96m'      # Cyan
    SUCCESS = '\033[1;32m' # Bold Green
    DEBUG = '\033[2;37m'   # Dim White
    RESET = '\033[0m'


class EnhancedResourceAnalyzer:
    """Enhanced analyzer for project type and cloud service requirements"""

    def __init__(self, repo_url: str, verbose: bool = False, model: str = 'deepseek', session_dir=None, use_api: bool = True):
        self.repo_url = repo_url
        self.verbose = verbose
        self.model = model
        self.session_dir = session_dir
        self.use_api = use_api  # Use GitHub API instead of cloning
        self.analysis_data = {}

    def log(self, message: str):
        """Log message if verbose mode is enabled"""
        if self.verbose:
            print(f"[ResourceAnalyzer] {message}")

    def analyze(self) -> CloudServiceRequirement:
        """
        Perform complete analysis and return cloud service requirements

        Returns:
            CloudServiceRequirement object with project type and required services
        """
        if self.use_api:
            print(f"{LogColors.INFO}  ðŸ” Fetching repository files via GitHub API...{LogColors.RESET}")
        else:
            print(f"{LogColors.INFO}  ðŸ” Cloning and analyzing repository...{LogColors.RESET}")
        print(f"{LogColors.DEBUG}     Repository: {self.repo_url}{LogColors.RESET}")

        # Step 1: Get repository files (via API or clone)
        temp_dir = None
        if self.use_api:
            temp_dir = self._fetch_via_api()
        else:
            temp_dir = self._clone_repository()

        if not temp_dir:
            print(f"{LogColors.DEBUG}  âš ï¸  Failed to access repository, using default configuration{LogColors.RESET}")
            return self._create_default_requirement()

        try:
            # Step 2: Analyze repository structure
            self._analyze_repository_files(temp_dir)

            # Step 3: Read README
            readme_content = self._read_readme(temp_dir)
            if readme_content:
                self.analysis_data['readme'] = readme_content[:5000]

            # Step 4: Read key files for AI analysis
            self._read_key_files(temp_dir)

            # Step 5: Try AI analysis first (if available)
            print(f"{LogColors.INFO}  ðŸ¤– Attempting AI-powered analysis...{LogColors.RESET}")
            ai_requirement = self._ai_analyze_comprehensive(temp_dir)

            if ai_requirement:
                print(f"{LogColors.SUCCESS}  âœ… AI analysis successful{LogColors.RESET}")
                return ai_requirement

            # Step 6: Fallback to rule-based analysis
            print(f"{LogColors.INFO}  âš™ï¸  Using rule-based analysis...{LogColors.RESET}")

            # Detect project type (comprehensive classification)
            project_type, subtype, features = self._detect_project_type_comprehensive(temp_dir)
            self.analysis_data['project_type'] = project_type
            self.analysis_data['subtype'] = subtype
            self.analysis_data['features'] = features

            # Determine required cloud services
            required_services = self._determine_cloud_services(project_type, features, temp_dir)

            # Generate CVM and database configurations
            cvm_config, db_config = self._generate_service_configs(project_type, features)

            # Detect primary language
            primary_language = self._detect_primary_language()

            # Calculate confidence
            confidence = self._calculate_confidence(features)

            # Create requirement object
            requirement = CloudServiceRequirement(
                project_type=project_type,
                project_subtype=subtype,
                primary_language=primary_language,
                required_services=required_services,
                cvm_config=cvm_config,
                database_config=db_config,
                confidence=confidence,
                analysis_reasoning=self._build_reasoning(project_type, features),
                detected_features=features
            )

            print(f"\n{LogColors.SUCCESS}  âœ… Analysis completed{LogColors.RESET}")
            print(f"{LogColors.DEBUG}     Project Type: {project_type.value}{LogColors.RESET}")
            if subtype:
                print(f"{LogColors.DEBUG}     Subtype: {subtype}{LogColors.RESET}")
            print(f"{LogColors.DEBUG}     Confidence: {confidence:.1%}{LogColors.RESET}")
            print(f"{LogColors.DEBUG}     Required Services: {len(required_services)}{LogColors.RESET}")

            return requirement

        finally:
            # Cleanup
            self.log(f"Keeping temp dir for debugging: {temp_dir}")

    def _fetch_via_api(self) -> Optional[str]:
        """
        Fetch repository files via GitHub API instead of cloning

        Returns:
            Path to directory containing fetched files, or None on failure
        """
        try:
            # Use session directory if provided, otherwise use temp directory
            if self.session_dir:
                temp_dir = str(Path(self.session_dir) / "repo_api_fetch")
                Path(temp_dir).mkdir(parents=True, exist_ok=True)
            else:
                temp_dir = tempfile.mkdtemp(prefix="gitcloud_api_")
            self.log(f"Fetching files to {temp_dir}")

            # Create GitHub fetcher
            fetcher = GitHubFetcher(self.repo_url, verbose=self.verbose)

            # Fetch all analysis files
            fetched_files = fetcher.fetch_analysis_files(output_dir=temp_dir)

            if len(fetched_files) > 0:
                self.log(f"Successfully fetched {len(fetched_files)} files via API")
                print(f"{LogColors.SUCCESS}  âœ… Fetched {len(fetched_files)} files via GitHub API{LogColors.RESET}")
                return temp_dir
            else:
                self.log("No files fetched via API, falling back to clone")
                print(f"{LogColors.DEBUG}  âš ï¸  API fetch returned no files, trying clone...{LogColors.RESET}")
                return self._clone_repository()

        except Exception as e:
            self.log(f"Error fetching via API: {e}, falling back to clone")
            print(f"{LogColors.DEBUG}  âš ï¸  API fetch failed, trying clone...{LogColors.RESET}")
            return self._clone_repository()

    def _clone_repository(self) -> Optional[str]:
        """Clone repository to temporary directory"""
        try:
            # Use session directory if provided, otherwise use temp directory
            if self.session_dir:
                temp_dir = str(Path(self.session_dir) / "repo_clone")
                Path(temp_dir).mkdir(parents=True, exist_ok=True)
            else:
                temp_dir = tempfile.mkdtemp(prefix="gitcloud_analysis_")
            self.log(f"Cloning repository to {temp_dir}")

            result = subprocess.run(
                ["git", "clone", "--depth", "1", self.repo_url, temp_dir],
                capture_output=True,
                timeout=60,
                text=True
            )

            if result.returncode == 0:
                self.log("Repository cloned successfully")
                print(f"{LogColors.SUCCESS}  âœ… Repository cloned successfully{LogColors.RESET}")
                return temp_dir
            else:
                self.log(f"Failed to clone: {result.stderr}")
                return None

        except Exception as e:
            self.log(f"Error cloning repository: {e}")
            return None

    def _analyze_repository_files(self, repo_path: str):
        """Analyze repository file structure with comprehensive detection"""
        self.log("Analyzing repository files")

        # æ‰©å±•çš„æ–‡ä»¶æ¨¡å¼æ£€æµ‹
        file_patterns = {
            # Python
            'requirements.txt': 'python_deps',
            'setup.py': 'python_setup',
            'pyproject.toml': 'python_modern',
            'Pipfile': 'python_pipenv',
            'conda.yml': 'python_conda',
            'environment.yml': 'python_conda',

            # Node.js
            'package.json': 'nodejs',
            'yarn.lock': 'nodejs_yarn',
            'pnpm-lock.yaml': 'nodejs_pnpm',

            # Java
            'pom.xml': 'java_maven',
            'build.gradle': 'java_gradle',
            'build.gradle.kts': 'java_gradle_kotlin',

            # Go
            'go.mod': 'golang',
            'go.sum': 'golang',

            # Rust
            'Cargo.toml': 'rust',

            # PHP
            'composer.json': 'php',

            # Ruby
            'Gemfile': 'ruby',

            # .NET
            '*.csproj': 'dotnet',
            '*.sln': 'dotnet_solution',

            # å®¹å™¨åŒ–
            'Dockerfile': 'docker',
            'docker-compose.yml': 'docker_compose',
            'docker-compose.yaml': 'docker_compose',

            # Kubernetes
            '*.yaml': 'k8s_config',
            'helm': 'helm',

            # æ•°æ®åº“
            '*.sql': 'sql_files',
            'migrations': 'db_migrations',

            # ML/AI
            '*.ipynb': 'jupyter',
            'train.py': 'ml_training',
            'model.py': 'ml_model',
            'inference.py': 'ml_inference',
            'requirements-ml.txt': 'ml_requirements',

            # å‰ç«¯æ¡†æž¶
            'angular.json': 'angular',
            'vue.config.js': 'vue',
            'next.config.js': 'nextjs',
            'nuxt.config.js': 'nuxtjs',
            'gatsby-config.js': 'gatsby',
            'svelte.config.js': 'svelte',

            # ç§»åŠ¨ç«¯
            'android': 'android',
            'ios': 'ios',
            'pubspec.yaml': 'flutter',
            'capacitor.config.json': 'capacitor',

            # CI/CD
            '.github/workflows': 'github_actions',
            '.gitlab-ci.yml': 'gitlab_ci',
            'Jenkinsfile': 'jenkins',

            # é…ç½®
            'terraform': 'terraform',
            'ansible': 'ansible',
        }

        found_files = []
        repo_path_obj = Path(repo_path)

        for pattern, file_type in file_patterns.items():
            if '*' in pattern or pattern in ['migrations', 'android', 'ios', 'helm', 'terraform', 'ansible']:
                # Directory or glob pattern
                matches = list(repo_path_obj.rglob(pattern))
            else:
                # Exact filename
                matches = list(repo_path_obj.rglob(pattern))

            if matches:
                found_files.append(file_type)
                self.log(f"Found {file_type}: {len(matches)} items")

        self.analysis_data['found_files'] = found_files

        # Count files
        try:
            file_count = sum(1 for _ in repo_path_obj.rglob('*') if _.is_file())
            self.analysis_data['file_count'] = file_count
        except Exception:
            pass

    def _read_readme(self, repo_path: str) -> Optional[str]:
        """Read README file"""
        readme_names = ['README.md', 'README.rst', 'README.txt', 'README', 'readme.md']

        for readme_name in readme_names:
            readme_path = Path(repo_path) / readme_name
            if readme_path.exists():
                try:
                    with open(readme_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        self.log(f"Read README: {len(content)} characters")
                        return content
                except Exception as e:
                    self.log(f"Error reading README: {e}")

        return None

    def _read_key_files(self, repo_path: str):
        """Read key configuration files for AI analysis"""
        key_files = {
            'requirements.txt': 'python_deps',
            'package.json': 'node_deps',
            'go.mod': 'go_deps',
            'pom.xml': 'java_deps',
            'Cargo.toml': 'rust_deps',
            'Dockerfile': 'docker',
            'docker-compose.yml': 'docker_compose',
            '.env.example': 'env_example',
            'requirements-dev.txt': 'dev_deps'
        }

        for filename, key in key_files.items():
            file_path = Path(repo_path) / filename
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        self.analysis_data[key] = content[:3000]  # Limit to 3000 chars
                        self.log(f"Read {filename}: {len(content)} characters")
                except Exception as e:
                    self.log(f"Error reading {filename}: {e}")

    def _ai_analyze_comprehensive(self, repo_path: str) -> Optional[CloudServiceRequirement]:
        """
        Use AI (Anthropic Claude) to analyze project and determine requirements

        Returns:
            CloudServiceRequirement if AI analysis succeeds, None otherwise
        """
        try:
            import anthropic
        except ImportError:
            self.log("Anthropic package not installed, skipping AI analysis")
            return None

        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            self.log("ANTHROPIC_API_KEY not set, skipping AI analysis")
            return None

        try:
            # Prepare analysis context
            context_parts = []

            # Add README
            if 'readme' in self.analysis_data:
                context_parts.append(f"READMEå†…å®¹:\n{self.analysis_data['readme']}")

            # Add dependencies
            if 'python_deps' in self.analysis_data:
                context_parts.append(f"\nrequirements.txt:\n{self.analysis_data['python_deps']}")
            if 'node_deps' in self.analysis_data:
                context_parts.append(f"\npackage.json:\n{self.analysis_data['node_deps']}")
            if 'go_deps' in self.analysis_data:
                context_parts.append(f"\ngo.mod:\n{self.analysis_data['go_deps']}")

            # Add Docker files
            if 'docker' in self.analysis_data:
                context_parts.append(f"\nDockerfile:\n{self.analysis_data['docker']}")
            if 'docker_compose' in self.analysis_data:
                context_parts.append(f"\ndocker-compose.yml:\n{self.analysis_data['docker_compose']}")

            # Add found files summary
            if 'found_files' in self.analysis_data:
                context_parts.append(f"\næ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»åž‹: {', '.join(self.analysis_data['found_files'])}")

            context = "\n".join(context_parts)

            # Project type options
            project_types = [t.value for t in ProjectType]

            # Construct AI prompt
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªäº‘èµ„æºåˆ†æžä¸“å®¶ã€‚è¯·åˆ†æžä»¥ä¸‹GitHubé¡¹ç›®ï¼Œåˆ¤æ–­é¡¹ç›®ç±»åž‹å’Œæ‰€éœ€äº‘æœåŠ¡èµ„æºã€‚

é¡¹ç›®ä¿¡æ¯:
{context}

è¯·åˆ†æžå¹¶è¿”å›žJSONæ ¼å¼ç»“æžœï¼ˆä»…è¿”å›žJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰:
{{
  "project_type": "é€‰æ‹©ä¸€ä¸ª: {', '.join(project_types)}",
  "project_subtype": "é¡¹ç›®å­ç±»åž‹æè¿°ï¼ˆå¯é€‰ï¼‰",
  "primary_language": "ä¸»è¦ç¼–ç¨‹è¯­è¨€ï¼ˆgolang/python/nodejs/java/rustç­‰ï¼‰",
  "needs_gpu": true/false,
  "gpu_type": "T4/V100/A10/A100/none",
  "cpu_cores": æ•°å­—,
  "memory_gb": æ•°å­—,
  "disk_gb": æ•°å­—,
  "needs_mysql": true/false,
  "needs_redis": true/false,
  "needs_object_storage": true/false,
  "needs_cdn": true/false,
  "reasoning": "åˆ†æžåŽŸå› "
}}

æ³¨æ„ï¼š
- project_type å¿…é¡»ä»Žä¸Šè¿°åˆ—è¡¨ä¸­é€‰æ‹©
- primary_language è¦æ ¹æ®æ£€æµ‹åˆ°çš„ä¾èµ–æ–‡ä»¶å’Œä»£ç åˆ¤æ–­
- GPUç±»åž‹ç”¨äºŽæœºå™¨å­¦ä¹ é¡¹ç›®ï¼šT4(å…¥é—¨)ã€V100(ä¸­ç«¯)ã€A10(é«˜ç«¯)ã€A100(é¡¶çº§)
- CPU/å†…å­˜/ç£ç›˜è¦æ ¹æ®é¡¹ç›®è§„æ¨¡åˆç†ä¼°ç®—
- æ•°æ®åº“ã€ç¼“å­˜ç­‰æ ¹æ®é¡¹ç›®å®žé™…éœ€æ±‚åˆ¤æ–­"""

            # Call AI API based on selected model
            if self.model == 'deepseek':
                client = anthropic.Anthropic(api_key=api_key, base_url="https://api.deepseek.com/anthropic")
                model_name = "deepseek-chat"
            else:  # anthropic
                client = anthropic.Anthropic(api_key=api_key)
                model_name = "claude-sonnet-4-20250514"

            message = client.messages.create(
                model=model_name,
                max_tokens=1000,
                temperature=0.0,
                messages=[{"role": "user", "content": prompt}]
            )

            response_text = message.content[0].text.strip()

            # Remove markdown code blocks if present
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            # Parse JSON
            result = json.loads(response_text)

            # Validate and construct CloudServiceRequirement
            project_type_str = result.get('project_type', 'WEB_APPLICATION')
            try:
                project_type = ProjectType(project_type_str)
            except ValueError:
                # Try to match by value
                project_type = ProjectType.WEB_APPLICATION
                for pt in ProjectType:
                    if pt.value == project_type_str:
                        project_type = pt
                        break

            # Build required services list
            required_services = []

            # CVM config
            cvm_config = ServiceRequirement(
                service_type=CloudServiceType.CVM,
                cpu_cores=result.get('cpu_cores', 2),
                memory_gb=result.get('memory_gb', 4),
                disk_gb=max(result.get('disk_gb', 100), 50),  # Ensure minimum 50GB
                gpu_required=result.get('needs_gpu', False),
                gpu_type=result.get('gpu_type') if result.get('needs_gpu') else None
            )
            required_services.append(cvm_config)

            # MySQL
            if result.get('needs_mysql', False):
                mysql_config = ServiceRequirement(
                    service_type=CloudServiceType.MYSQL,
                    cpu_cores=2,
                    memory_gb=4,
                    disk_gb=100
                )
                required_services.append(mysql_config)

            # Redis
            if result.get('needs_redis', False):
                redis_config = ServiceRequirement(
                    service_type=CloudServiceType.REDIS,
                    memory_gb=2
                )
                required_services.append(redis_config)

            # Object Storage
            if result.get('needs_object_storage', False):
                storage_config = ServiceRequirement(
                    service_type=CloudServiceType.OBJECT_STORAGE,
                    disk_gb=500
                )
                required_services.append(storage_config)

            # CDN
            if result.get('needs_cdn', False):
                cdn_config = ServiceRequirement(
                    service_type=CloudServiceType.CDN
                )
                required_services.append(cdn_config)

            # Create requirement object
            requirement = CloudServiceRequirement(
                project_type=project_type,
                project_subtype=result.get('project_subtype'),
                primary_language=result.get('primary_language'),
                required_services=required_services,
                analysis_reasoning=result.get('reasoning', 'AIåˆ†æž'),
                confidence=0.9  # High confidence for AI analysis
            )

            return requirement

        except Exception as e:
            self.log(f"AI analysis failed: {e}")
            import traceback
            if self.verbose:
                traceback.print_exc()
            return None

    def _detect_project_type_comprehensive(self, repo_path: str) -> Tuple[ProjectType, Optional[str], List[str]]:
        """
        Comprehensive project type detection with subtype classification

        Returns:
            Tuple of (ProjectType, subtype, detected_features)
        """
        found_files = self.analysis_data.get('found_files', [])
        readme = self.analysis_data.get('readme', '').lower()
        features = []

        # === ML/AI é¡¹ç›®æ£€æµ‹ ===
        ml_indicators = ['ml_training', 'ml_model', 'ml_inference', 'jupyter', 'ml_requirements']
        if any(ind in found_files for ind in ml_indicators):
            features.append('machine_learning')

            # Check for LLM-specific keywords
            llm_keywords = ['llm', 'gpt', 'bert', 'transformer', 'huggingface', 'openai', 'chatgpt']
            if any(kw in readme for kw in llm_keywords):
                features.append('large_language_model')
                return ProjectType.LLM_SERVICE, 'LLMæŽ¨ç†æœåŠ¡', features

            # Check for training vs inference
            if 'ml_training' in found_files or 'train' in readme:
                features.append('training')
                return ProjectType.ML_TRAINING, 'MLæ¨¡åž‹è®­ç»ƒ', features
            else:
                features.append('inference')
                return ProjectType.ML_INFERENCE, 'MLæ¨¡åž‹æŽ¨ç†', features

        # Check Python ML dependencies
        if 'python_deps' in found_files:
            req_path = Path(repo_path) / 'requirements.txt'
            if req_path.exists():
                try:
                    content = req_path.read_text().lower()
                    ml_packages = ['tensorflow', 'torch', 'pytorch', 'keras', 'sklearn',
                                   'transformers', 'numpy', 'pandas', 'scipy', 'jax']
                    if any(pkg in content for pkg in ml_packages):
                        features.append('machine_learning')
                        return ProjectType.ML_INFERENCE, 'Python MLåº”ç”¨', features
                except Exception:
                    pass

        # === æ•°æ®å¤„ç†é¡¹ç›®æ£€æµ‹ ===
        data_keywords = ['spark', 'hadoop', 'airflow', 'luigi', 'dask', 'flink', 'kafka']
        if any(kw in readme for kw in data_keywords):
            features.append('big_data')
            if 'spark' in readme or 'hadoop' in readme:
                return ProjectType.BIG_DATA, 'Spark/Hadoopå¤§æ•°æ®', features
            elif 'kafka' in readme or 'flink' in readme:
                return ProjectType.STREAM_PROCESSING, 'æµå¤„ç†', features
            else:
                return ProjectType.DATA_ETL, 'ETLæ•°æ®å¤„ç†', features

        # === å‰ç«¯é¡¹ç›®æ£€æµ‹ ===
        frontend_frameworks = ['angular', 'vue', 'nextjs', 'nuxtjs', 'gatsby', 'svelte']
        if any(fw in found_files for fw in frontend_frameworks):
            features.append('frontend_framework')
            return ProjectType.WEB_FRONTEND, 'å‰ç«¯åº”ç”¨', features

        # Check for static site
        if 'nodejs' in found_files and 'package.json' in Path(repo_path).rglob('*'):
            try:
                pkg_json = Path(repo_path) / 'package.json'
                if pkg_json.exists():
                    pkg_content = pkg_json.read_text()
                    if 'gatsby' in pkg_content or 'next' in pkg_content or 'vite' in pkg_content:
                        features.append('static_site')
                        return ProjectType.WEB_STATIC, 'é™æ€ç½‘ç«™', features
            except:
                pass

        # === åŽç«¯ API æ£€æµ‹ ===
        # Check for API frameworks
        api_indicators = {
            'flask': 'Flask API',
            'fastapi': 'FastAPI',
            'django': 'Django',
            'express': 'Express.js',
            'koa': 'Koa.js',
            'spring': 'Spring Boot',
            'gin': 'Gin (Go)',
            'echo': 'Echo (Go)',
            'actix': 'Actix (Rust)',
        }

        for framework, description in api_indicators.items():
            if framework in readme or framework in str(found_files):
                features.append('backend_api')
                features.append(framework)
                return ProjectType.WEB_BACKEND, description, features

        # === å¾®æœåŠ¡æ£€æµ‹ ===
        if 'k8s_config' in found_files or 'helm' in found_files or 'docker_compose' in found_files:
            microservice_keywords = ['microservice', 'grpc', 'service mesh', 'istio']
            if any(kw in readme for kw in microservice_keywords):
                features.append('microservices')
                return ProjectType.MICROSERVICES, 'å¾®æœåŠ¡æž¶æž„', features

        # === å…¨æ ˆé¡¹ç›®æ£€æµ‹ ===
        has_frontend = any(fw in found_files for fw in ['angular', 'vue', 'nodejs'])
        has_backend = 'python_deps' in found_files or 'java_maven' in found_files
        if has_frontend and has_backend:
            features.append('fullstack')
            return ProjectType.WEB_FULLSTACK, 'å…¨æ ˆåº”ç”¨', features

        # === ç§»åŠ¨åŽç«¯æ£€æµ‹ ===
        mobile_keywords = ['mobile', 'ios', 'android', 'flutter', 'react native']
        if any(kw in readme for kw in mobile_keywords):
            features.append('mobile_backend')
            return ProjectType.MOBILE_BACKEND, 'ç§»åŠ¨åº”ç”¨åŽç«¯', features

        # === ç”µå•†é¡¹ç›®æ£€æµ‹ ===
        ecommerce_keywords = ['ecommerce', 'e-commerce', 'shopping', 'cart', 'payment', 'order']
        if any(kw in readme for kw in ecommerce_keywords):
            features.append('ecommerce')
            return ProjectType.ECOMMERCE, 'ç”µå•†å¹³å°', features

        # === æ¸¸æˆæœåŠ¡å™¨æ£€æµ‹ ===
        game_keywords = ['game', 'unity', 'unreal', 'multiplayer', 'mmo']
        if any(kw in readme for kw in game_keywords):
            features.append('game_server')
            return ProjectType.GAME_SERVER, 'æ¸¸æˆæœåŠ¡å™¨', features

        # === CMS æ£€æµ‹ ===
        cms_keywords = ['cms', 'content management', 'wordpress', 'strapi', 'ghost']
        if any(kw in readme for kw in cms_keywords):
            features.append('cms')
            return ProjectType.CONTENT_MANAGEMENT, 'CMSç³»ç»Ÿ', features

        # === æ•°æ®åº“åº”ç”¨æ£€æµ‹ ===
        if 'sql_files' in found_files or 'db_migrations' in found_files:
            features.append('database_heavy')
            return ProjectType.DATABASE_APP, 'æ•°æ®åº“åº”ç”¨', features

        # === CI/CD å·¥å…·æ£€æµ‹ ===
        if any(ci in found_files for ci in ['github_actions', 'gitlab_ci', 'jenkins']):
            features.append('cicd')
            return ProjectType.CI_CD, 'CI/CDç³»ç»Ÿ', features

        # === é»˜è®¤ï¼šWeb åº”ç”¨æˆ–é€šç”¨åº”ç”¨ ===
        if 'nodejs' in found_files or 'python_deps' in found_files or 'golang' in found_files:
            features.append('web_application')
            return ProjectType.WEB_BACKEND, 'WebåŽç«¯åº”ç”¨', features

        return ProjectType.GENERAL, None, features

    def _determine_cloud_services(
        self,
        project_type: ProjectType,
        features: List[str],
        repo_path: str
    ) -> List[ServiceRequirement]:
        """
        Determine required cloud services based on project type and features
        """
        required_services = []

        # Get default services for this project type
        default_services = get_default_services_for_project(project_type)

        # CVM (äº‘æœåŠ¡å™¨) - å‡ ä¹Žæ‰€æœ‰é¡¹ç›®éƒ½éœ€è¦
        if CloudServiceType.CVM in default_services or project_type != ProjectType.SERVERLESS:
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.CVM,
                required=True,
                reason="è¿è¡Œåº”ç”¨ç¨‹åºä¸»æœåŠ¡"
            ))

        # MySQL æ•°æ®åº“
        if self._needs_mysql(project_type, features, repo_path):
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.MYSQL,
                required=True,
                reason="æŒä¹…åŒ–å­˜å‚¨ä¸šåŠ¡æ•°æ®"
            ))

        # Redis ç¼“å­˜
        if self._needs_redis(project_type, features):
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.REDIS,
                required=False,
                reason="ç¼“å­˜çƒ­ç‚¹æ•°æ®ï¼Œæå‡æ€§èƒ½"
            ))

        # å¯¹è±¡å­˜å‚¨
        if self._needs_object_storage(project_type, features):
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.OBJECT_STORAGE,
                required=False,
                reason="å­˜å‚¨æ–‡ä»¶ã€å›¾ç‰‡ã€è§†é¢‘ç­‰é™æ€èµ„æº"
            ))

        # CDN
        if self._needs_cdn(project_type, features):
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.CDN,
                required=False,
                reason="åŠ é€Ÿé™æ€èµ„æºè®¿é—®"
            ))

        # è´Ÿè½½å‡è¡¡
        if self._needs_load_balancer(project_type, features):
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.LOAD_BALANCER,
                required=False,
                reason="åˆ†å‘æµé‡ï¼Œæé«˜å¯ç”¨æ€§"
            ))

        # GPU è®¡ç®—
        if 'machine_learning' in features or project_type in [
            ProjectType.ML_TRAINING, ProjectType.ML_INFERENCE, ProjectType.LLM_SERVICE
        ]:
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.GPU_COMPUTE,
                required=True,
                reason="GPUåŠ é€Ÿæ¨¡åž‹è®­ç»ƒ/æŽ¨ç†"
            ))

        # Kubernetes
        if 'microservices' in features or 'k8s_config' in self.analysis_data.get('found_files', []):
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.KUBERNETES,
                required=False,
                reason="å®¹å™¨ç¼–æŽ’å’Œå¾®æœåŠ¡ç®¡ç†"
            ))

        # æ¶ˆæ¯é˜Ÿåˆ—
        if project_type in [ProjectType.MICROSERVICES, ProjectType.BIG_DATA, ProjectType.STREAM_PROCESSING]:
            required_services.append(ServiceRequirement(
                service_type=CloudServiceType.MESSAGE_QUEUE,
                required=False,
                reason="å¼‚æ­¥æ¶ˆæ¯å¤„ç†"
            ))

        return required_services

    def _needs_mysql(self, project_type: ProjectType, features: List[str], repo_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ MySQL"""
        # æ˜Žç¡®éœ€è¦æ•°æ®åº“çš„é¡¹ç›®ç±»åž‹
        needs_db_types = [
            ProjectType.WEB_BACKEND,
            ProjectType.WEB_FULLSTACK,
            ProjectType.DATABASE_APP,
            ProjectType.MICROSERVICES,
            ProjectType.MOBILE_BACKEND,
            ProjectType.ECOMMERCE,
            ProjectType.CRM_ERP,
            ProjectType.CONTENT_MANAGEMENT,
        ]

        if project_type in needs_db_types:
            return True

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åº“é…ç½®æ–‡ä»¶
        found_files = self.analysis_data.get('found_files', [])
        if 'sql_files' in found_files or 'db_migrations' in found_files:
            return True

        # æ£€æŸ¥ä¾èµ–ä¸­æ˜¯å¦æœ‰æ•°æ®åº“é©±åŠ¨
        if 'python_deps' in found_files:
            req_path = Path(repo_path) / 'requirements.txt'
            if req_path.exists():
                try:
                    content = req_path.read_text().lower()
                    db_packages = ['mysql', 'pymysql', 'mysqlclient', 'sqlalchemy', 'django', 'flask-sqlalchemy']
                    if any(pkg in content for pkg in db_packages):
                        return True
                except:
                    pass

        return False

    def _needs_redis(self, project_type: ProjectType, features: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ Redis"""
        # é«˜æµé‡ã€éœ€è¦ç¼“å­˜çš„é¡¹ç›®ç±»åž‹
        return project_type in [
            ProjectType.WEB_BACKEND,
            ProjectType.WEB_FULLSTACK,
            ProjectType.MICROSERVICES,
            ProjectType.MOBILE_BACKEND,
            ProjectType.ECOMMERCE,
            ProjectType.LLM_SERVICE,
        ]

    def _needs_object_storage(self, project_type: ProjectType, features: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹è±¡å­˜å‚¨"""
        return project_type in [
            ProjectType.WEB_FULLSTACK,
            ProjectType.MOBILE_BACKEND,
            ProjectType.ECOMMERCE,
            ProjectType.CONTENT_MANAGEMENT,
            ProjectType.ML_TRAINING,
            ProjectType.DATA_ETL,
            ProjectType.BIG_DATA,
        ]

    def _needs_cdn(self, project_type: ProjectType, features: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ CDN"""
        return project_type in [
            ProjectType.WEB_FRONTEND,
            ProjectType.WEB_STATIC,
            ProjectType.WEB_FULLSTACK,
            ProjectType.ECOMMERCE,
            ProjectType.CONTENT_MANAGEMENT,
        ]

    def _needs_load_balancer(self, project_type: ProjectType, features: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è´Ÿè½½å‡è¡¡"""
        return project_type in [
            ProjectType.MICROSERVICES,
            ProjectType.ECOMMERCE,
            ProjectType.LLM_SERVICE,
        ] or 'high_traffic' in features

    def _generate_service_configs(
        self,
        project_type: ProjectType,
        features: List[str]
    ) -> Tuple[Optional[Dict], Optional[Dict]]:
        """
        Generate CVM and database configurations based on project type

        Returns:
            Tuple of (cvm_config, database_config)
        """
        cvm_config = None
        db_config = None

        # === CVM é…ç½® ===
        if project_type in [ProjectType.ML_TRAINING, ProjectType.ML_INFERENCE, ProjectType.LLM_SERVICE]:
            # ML/AI é¡¹ç›®éœ€è¦ GPU
            gpu_type = 'T4' if project_type == ProjectType.ML_INFERENCE else 'V100'
            cvm_config = {
                'cpu_cores': 8 if project_type == ProjectType.ML_INFERENCE else 16,
                'memory_gb': 32 if project_type == ProjectType.ML_INFERENCE else 64,
                'disk_gb': 200 if project_type == ProjectType.ML_INFERENCE else 500,
                'gpu_type': gpu_type,
            }
        elif project_type in [ProjectType.BIG_DATA, ProjectType.DATA_ETL]:
            # æ•°æ®å¤„ç†é¡¹ç›®
            cvm_config = {
                'cpu_cores': 16,
                'memory_gb': 64,
                'disk_gb': 500,
            }
        elif project_type in [ProjectType.ECOMMERCE, ProjectType.MICROSERVICES]:
            # é«˜æµé‡é¡¹ç›®
            cvm_config = {
                'cpu_cores': 8,
                'memory_gb': 16,
                'disk_gb': 200,
            }
        else:
            # é»˜è®¤é…ç½®
            cvm_config = {
                'cpu_cores': 2,
                'memory_gb': 4,
                'disk_gb': 100,
            }

        # === æ•°æ®åº“é…ç½® ===
        if 'database_heavy' in features or project_type in [ProjectType.DATABASE_APP, ProjectType.ECOMMERCE]:
            db_config = {
                'cpu_cores': 4,
                'memory_mb': 8000,
                'storage_gb': 200,
                'version': '8.0',
            }
        elif project_type in [ProjectType.WEB_BACKEND, ProjectType.WEB_FULLSTACK, ProjectType.MOBILE_BACKEND]:
            db_config = {
                'cpu_cores': 2,
                'memory_mb': 4000,
                'storage_gb': 100,
                'version': '8.0',
            }

        return cvm_config, db_config

    def _detect_primary_language(self) -> Optional[str]:
        """
        æ£€æµ‹é¡¹ç›®çš„ä¸»è¦ç¼–ç¨‹è¯­è¨€

        Returns:
            ä¸»è¦ç¼–ç¨‹è¯­è¨€å­—ç¬¦ä¸²ï¼ˆå¦‚ 'golang', 'python', 'nodejs'ï¼‰
        """
        found_files = self.analysis_data.get('found_files', [])

        # è¯­è¨€ä¼˜å…ˆçº§æ˜ å°„ï¼ˆæŒ‰ç‰¹å¼‚æ€§æŽ’åºï¼‰
        language_priority = {
            'golang': 10,
            'rust': 9,
            'java_maven': 8,
            'java_gradle': 8,
            'nodejs': 7,
            'python_modern': 6,
            'python_setup': 5,
            'python_deps': 4,
            'php': 3,
            'ruby': 2,
        }

        # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„è¯­è¨€
        best_language = None
        best_priority = -1

        for file_type in found_files:
            priority = language_priority.get(file_type, 0)
            if priority > best_priority:
                best_priority = priority
                best_language = file_type

        # ç»Ÿä¸€è¿”å›žå€¼
        if best_language:
            if best_language == 'golang':
                return 'golang'
            elif best_language in ['java_maven', 'java_gradle']:
                return 'java'
            elif best_language == 'nodejs':
                return 'nodejs'
            elif best_language in ['python_modern', 'python_setup', 'python_deps']:
                return 'python'
            elif best_language == 'rust':
                return 'rust'
            elif best_language == 'php':
                return 'php'
            elif best_language == 'ruby':
                return 'ruby'

        return None

    def _calculate_confidence(self, features: List[str]) -> float:
        """Calculate confidence based on detected features"""
        base_confidence = 0.5

        # More features = higher confidence
        if len(features) >= 5:
            base_confidence = 0.9
        elif len(features) >= 3:
            base_confidence = 0.8
        elif len(features) >= 2:
            base_confidence = 0.7
        elif len(features) >= 1:
            base_confidence = 0.6

        return base_confidence

    def _build_reasoning(self, project_type: ProjectType, features: List[str]) -> str:
        """Build human-readable reasoning"""
        lines = [
            f"æ£€æµ‹åˆ°é¡¹ç›®ç±»åž‹ä¸º {project_type.value}",
            f"æ£€æµ‹åˆ°çš„ç‰¹å¾: {', '.join(features) if features else 'æ— '}",
        ]
        return " | ".join(lines)

    def _create_default_requirement(self) -> CloudServiceRequirement:
        """Create default requirement when analysis fails"""
        return CloudServiceRequirement(
            project_type=ProjectType.GENERAL,
            required_services=[
                ServiceRequirement(
                    service_type=CloudServiceType.CVM,
                    required=True,
                    reason="é»˜è®¤é…ç½®"
                )
            ],
            cvm_config={'cpu_cores': 2, 'memory_gb': 4, 'disk_gb': 100},
            confidence=0.3,
            analysis_reasoning="æ— æ³•åˆ†æžä»“åº“ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"
        )


def analyze_cloud_services(repo_url: str, verbose: bool = False, model: str = 'deepseek', session_dir=None, use_api: bool = True) -> CloudServiceRequirement:
    """
    Convenience function to analyze repository and determine cloud service requirements

    Args:
        repo_url: GitHub repository URL
        verbose: Enable verbose logging
        model: AI model to use ('deepseek' or 'anthropic')
        session_dir: Optional session directory to store cloned repository
        use_api: Use GitHub API instead of cloning (default: True)

    Returns:
        CloudServiceRequirement object
    """
    analyzer = EnhancedResourceAnalyzer(repo_url, verbose=verbose, model=model, session_dir=session_dir, use_api=use_api)
    return analyzer.analyze()
